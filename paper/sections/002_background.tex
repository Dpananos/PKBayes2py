\section{Background}\label{ss:background}

In the following two subsections, we present background material on dynamic treatment regimes, which are used to develop optimal decision-making models, and Bayesian PK models, which are used to capture relationships among patient characteristics, measurements, pharmacokinetics, and dose so that optimal dosing decisions can be derived using the dynamic treatment regime framework.

\subsection{Dynamic Treatment Regimes}

In this section, we review the theory of dynamic treatment regimes and how dynamic personalization using PK models can be formulated using a dynamic treatment regime.

Our work considers personalization of a dose or sequences of doses for a patient with the aim of keeping their blood serum concentration of a drug within a desired range for as long as possible given two practical constraints: first we are limited to baseline measurements of clinical variables (e.g. age, weight, and/or measures of kidney function) to make our initial dosing decision, and second the subject’s blood concentration cannot be measured very frequently after the initial dose. The theory of dynamic treatment regimes and statistical reinforcement learning offers a way of operationalizing optimal dynamic personalization that combines baseline information with subsequent blood concentration information to select initial and subsequent doses that optimize a chosen criterion(for example, the time the patient spends in therapeutic range).

A dynamic treatment regime (DTR) is a sequence of decision rules for adapting a treatment plan to the time-varying state of an individual subject \cite{chakraborty2013statistical}. In DTRs, and their cousin topic in computer science \textit{reinforcement learning}, an agent (often thought of as a robot in reinforcement learning, but within medicine sometimes thought of as a physician’s computerized decision support system) interacts with a system for a number of stages. At each stage, the agent receives an \textit{observation} of the system and then decides which \textit{action} to take.  This action will result in an observed \textit{reward} which is followed by a new observation of the system after it has been impacted by the action.  This cycle of observation, action, reward then repeats, with the agent aiming to take actions which yield the largest total reward.

\subsubsection{Trajectories}

The data generated by the cycle of observation, action, and reward from the initial action to the final reward is called a \textit{trajectory}. Formally, we define a stage to be a triple containing an observation, chosen action, and resulting reward. Let $O_i$ denote an observation at the $i^{th}$ stage, $ A_i $ be the action at the $ i^{th} $ stage, and $ Y_i $ denote the reward at the $ i^{th}$ stage, denoted in capital letters when considering the observation, action, and reward as random variables. Following notation by Chakraborty and Moodie \cite{chakraborty2013statistical},  define the history of the system at stage $j$ to be $ H_j = (O_1, A_1, O_2, A_2, \cdots , O_{j-1}, A_{j-1}, O_j) $.  The reward at stage $j$ can be thought of as a function of the system’s history, the action taken, and possibly the new state of the system $ Y_j = Y_j(H_j, A_j, O_{j+1}) $.  As we explain in the next section, the sum of rewards from each stage under different actions is of primary interest in DTRs.  Since the reward is a random variable, the sum of rewards is also a random variable.  We refer to the expectation of the sum of rewards as \textit{the value of a policy}, and we refer to the observed sum of rewards as \textit{the return of a policy}. Importantly, rewards reflect the immediate desirability of single action, where as value reflects longer term desirability of a sequence of actions.

\subsubsection{Policies, Value Functions, and Q-Learning}

A policy $ d = (d_1, \cdots, d_K) $ is a vector of decision rules each of which take as input the system’s history and output an action to take.  Each decision rule is a function $d_j : \mathcal{H}_j \to \mathcal{A}_j$ where $\mathcal{H}_j$ and $\mathcal{A}_j$ are the history and action spaces at stage $j$ respectively.  The stage $ j $ value function for a policy $ d $ is the expected sum of rewards the agent would receive starting from history $ h_j  $ (here in lower case since it is an observed quantity) if it chose actions according to $ d $ for every action thereafter.  The value function is written as
\begin{equation}
	V^d_j(h_j) = E_d\left[ \sum_{k=j}^K Y_k(H_k, A_k, O_{k+1}) \Bigg\lvert H_j = h_j\right] \>.
\end{equation}

\noindent Here, the expectation is over the distribution of trajectories. Importantly, the stage $ j $ 
value function can be decomposed into the expectation of reward at stage $ j $ plus the stage $ j+1  $ value function  \cite{chakraborty2013statistical}
\begin{equation}
V^d_j(h_j) = E_d\left[Y_j(H_j, A_j, O_{j+1}) + V^d_{j+1}(H_{j+1}) \vert H_j = h_j\right] \>.
\end{equation}


\noindent The optimal stage $ j  $ value function is the value function under a policy which yields maximal value

\begin{equation}
V^{opt}_j(h_j) = \underset{d \in \mathscr{D}}{\mbox{max}} V^d_j(h_j) \>.
\end{equation}

\noindent  Estimating a policy that maximizes value can be achieved by estimating the optimal Q function \cite{chakraborty2013statistical}.  The optimal Q function at stage $ j $ is a function of the system’s history $ h_j $ and a proposed action $ a_j $,
\begin{equation}
 Q_j^{opt}(h_j, a_j) = E \left[ 
 Y_j(H_j, A_j, O_{j+1}) + V^{opt}_{j+1}(H_{j+1}) \lvert H_j = h_j, A_j = a_j
 \right].
\end{equation}

Note that the optimal Q function has similar form and interpretation to the optimal value function (namely, it is the expected sum rewards \textemdash the value \textemdash starting at stage $ j $ but with the added condition that we take action $ a_j $ and then follow the optimal policy thereafter). Due to the decomposition of the reward function at stage $ j $, estimation of the optimal Q function can be performed by choosing the action which yields the largest reward at each stage assuming we act optimally in the future. 

Given the optimal Q function, an optimal policy is given by 
\begin{equation}
d_j^{opt}(h_j) = \arg\max_{a} Q_j^{opt}(h_j,a).
\end{equation}
To use Q-learning for personalization in the context of optimal dosing and titration, we will define the actions to be possible doses or dose adjustments, and we define the reward to be a function of the resulting concentrations which implicitly rely on the actions, for example a measurement of how well concentrations are kept in a specified therapeutic range. The relationship between possible actions and rewards, which Q-learning can use to produce optimal policies, can be captured by Bayesian PK modelling. We review Bayesian PK modelling in the next section.

\subsection{Bayesian Models of Pharmacokinetics}

In order to estimate the optimal Q functions, we need to be able to predict how a patient's concentration is likely to evolve over time in response to a hypothetical dose (action).  Our approach is to build a Bayesian model of patient pharmacokinetics that can use baseline clinical information, as well as any available concentration measurements, to make tailored predictions of future concentrations that are as accurate as possible given the model structure and available data. The model is flexible in that it can condition on whatever information is available---for example, if previous dose and measurement information is not available for a specific patient, the model will rely on baseline information alone. If it is available, the model will use it to (hopefully) make improved predictions. This allows us to optimize both initial doses and later dose adjustments after additional information about concentration is acquired.

Bayesian models have another key property that we use in our framework. They are able to simulate the trajectories of patients drawn from a distribution that is similar to the distribution of the data that the models were trained on, but in the simulated data, \textit{all} variables---including normally-hidden PK parameters---are fully observed. This allows us to conduct a form of internal validation where we use the simulated patients to assess the relative benefits of different modes of static and dynamic personalization, because we can know for each simulated patient exactly what the effect of any dose would be. This process is described in detail in the next section, where we present our framework, and the details of the Bayesian model itself are provided in Appendix~\ref{ap:appendix}.
